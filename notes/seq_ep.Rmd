---
title: "Sequence correction for Gaussian probability"
author: "Patrick Ding"
date: "`r Sys.Date()`"
output: 
  pdf_document:
    extra_dependencies: 
    - ["algorithm"]
    - ["algorithmicx"]
    - ["algpseudocode"]
    - ["float"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


Expectation propagation for Gaussian probability estimation works well for rectangular integration regions, but poorly for polyhedral regions. We provide a
way to rewrite the polyhedral region Gaussian probability problem as a rectangular problem.

# Cast polyhedral problem as rectangular one

We write the multivariate Gaussian density with covariance $\Sigma$ truncated to
a polytope region $l \le Ax \le u$, where $x \in \mathbb{R}^n$, 
$A \in \mathbb{R}^{m \times n}$, and $l,u \in \mathbb{R}^m$, as
\begin{align*}
  p(x) &= \mathcal{N}_n(x; 0, \Sigma) 1 \{l \le Cx \le u \}
  \\
  &= \mathcal{N}_n(x; 0, \Sigma) \prod_{i=1}^m t_i(x),
\end{align*}
where $t_i(x) = 1 \{l_i \le c_i^T x \le u_i \}$. 
For random variable $X$ with this density, the normalizing constant of this density is $P(l \le AX \le u)$. Let's assume $\Sigma = I$, and call the 
random variable $Z$ in this case, so we want $P(l \le AZ \le u)$. 

Let $A_{\cdot, k}$ be the $k$th column of $A$. Let $\epsilon_i \sim_{iid} \mathcal{N}(0, 1)$ for $i \in \{1, \ldots, n\}$. We construct a sequence of random
vectors $\{X_1, X_2, \ldots, X_n\}$ 
\begin{align*}
  X_1 &= A_{\cdot, 1} \epsilon_1 
  \\
  X_2 &= X_1 + A_{\cdot, 2} \epsilon_2
  \\
  \vdots
  \\
  X_{n-1} &= X_{n-2} + A_{\cdot, n-1} \epsilon_{n-1}
  \\
  X_n &= X_{n-1} + A_{\cdot, n} \epsilon_n.
\end{align*}
Substituting $X_k$ for $k \in \{1, \ldots, n-1\}$ into the last expression, observe that
\begin{align*}
  X_n &= A_{\cdot, 1} \epsilon_1 + \cdots + A_{\cdot, n} \epsilon_n
  \\
  &= A \begin{pmatrix} \epsilon_1 \\ \vdots \\ \epsilon_n  \end{pmatrix}.
\end{align*}
From properties of multivariate normal $X_n$ has the same distribution as $AZ$,
$\mathcal{N}_m(0, AA^T)$, so
\begin{align*}
  P(l \le X_n \le u) = P(l \le AZ \le u).
\end{align*}
The joint density of the $X_i$ factorizes as
\begin{align*}
  p(x_1, \ldots, x_n) &= p(x_n| x_{n-1})p(x_{n-1}|x_{n-2}) \cdots p(x_2|x_1)p(x_1)
\end{align*}
where
\begin{align*}
  p(x_i | x_{i-1}) &= \mathcal{N}_m(x_i; x_{i-1}, A_{\cdot, i}A^T_{\cdot, i}) & i \in \{2, \ldots, n\}
  \\
  p(x_1) &= \mathcal{N}_m(x_1; 0, A_{\cdot, 1}A^T_{\cdot, 1}).
\end{align*}
The covariance $\Xi_i = A_{\cdot, i}A_{\cdot, i}^T$ is not invertible in general, it is rank 1.
But from Khatri (1968) we can express this singular normal distribution in terms of the Moore-Penrose pseudoinverse of $\Xi_i$. 
Let $\gamma_i$ be the sole eigenvector of $\Xi_i$, and let $\lambda_i$ be the sole eigenvalue of $\Xi_i$.
Then 
\begin{align}
  \gamma_i &= \frac{A_{\cdot, i}}{||A_{\cdot, i}||}
  \\
  \lambda_i &= ||A_{\cdot, i}||^2
  \\
  \Xi^{+} &= \frac{1}{\lambda_i} \gamma_i \gamma_i^T,
\end{align}
and the density is
\begin{align*}
  p(x_i | x_{i-1}) &= (2\pi)^{-n/2} |\lambda_i|^{-1/2} 
    \exp \left[ -\frac{\lambda_i}{2}(x_i - x_{i-1})^T\gamma_i \gamma_i^T(x_i - x_{i-1})\right]
  \\
  p(x_1) &= (2\pi)^{-n/2} |\lambda_1|^{-1/2} 
    \exp \left[ -\frac{\lambda_1}{2}x_1^T\gamma_1 \gamma_1^T x_1 \right].
\end{align*}
Finally we add the indicator for the constraints on $X_n$ and get an augmented truncated multivariate normal density proportional to:
\begin{align}\label{eq:augmented}
  p(x) &= \frac{1}{S} p(x_1, \ldots, x_n)1\{\ell \le x_n \le u \} \nonumber
  \\
  &= \frac{1}{S} p(x_1) \left[ \prod_{i=2}^n p(x_i|x_{i-1}) \right] 1\{\ell \le x_n \le u \}, 
\end{align}
where $x = (x_1, \ldots, x_n)^T \in \mathbb{R}^{mn}$ and $S$ is a normalizing constant.
The ratio of this density's normalizing constant and the normalizing constant of $p(x_1, \ldots, x_n)$ will give us $P(\ell \le X_n \le u)$. 

# EP form

This density is intractable because of the term $1\{\ell \le x_n \le b \}$, but we can approximate the density with a Gaussian using the EP method of Cunningham et al (2011). 
This will be efficient and accurate since the truncation region is axis aligned, and their method performs well in that case. 
Equation \ref{eq:augmented} is exactly of the form that the Cunningham et al
method handles:
\begin{align}
  p(x) &= p_0(x) \prod_{k=1}^n t_k(x) 
\end{align}
where
\begin{align}\label{eq:ep_form}
  p_0(x) &= p(x_1) \left[ \prod_{i=2}^n p(x_i|x_{i-1}) \right] & \text{prior}
  \\
  t_k(x) &= 1 \{\ell_k \le c_k^T x \le b_k \} = 1 \{\ell_k \le x_{n_k} \le b_k \} & \text{likelihood factor}
  \\
  c_k &= 
    \begin{pmatrix} 
      0_{1 \times m(n-1)} & 0_{1 \times k-1} & 1 & 0_{1 \times m-k}
    \end{pmatrix}^T
\end{align}

The density of the prior is singular Gaussian, 
\begin{align*}
  p_0(x) &= \mathcal{N}_{nm}(x; 0, K)
  \\
  &= (2 \pi)^{-n/2} \left( \prod_{i=1}^n |\lambda_i|^{-1/2} \right) \exp(x^T K^{-1} x)
  \\
  K^{-1} &= 
    \begin{pmatrix}
      \Xi^{+}_1 + \Xi^{+}_2 & \Xi^+_2\
      \\
      \Xi^{+}_2 & \Xi^{+}_2 + \Xi^{+}_3 & \Xi^{+}_3 
      \\
      & \Xi^{+}_3 & \Xi^{+}_3  + \Xi^{+}_4
      \\
      & & & \ddots
      \\
      & & & & \Xi^{+}_{n-1} + \Xi^{+}_n & \Xi^{+}_n 
      \\
      & & & & \Xi^{+}_n & \Xi^{+}_n
    \end{pmatrix}
\end{align*}
where unspecified blocks in $K^{-1}$ are 0.

We also have access to the covariance matrix of $x$, $K$. Note that
\begin{align}
  X_i &= \sum_{k=1}^i A_{\cdot, k} \epsilon_k 
  \\
  E[X_i] &= 0,
\end{align}
and $K_{ij}$ is
\begin{align}
  Cov(X_i, X_j) &= 
    E \left[ 
      \left( \sum_{r = 1}^i A_{\cdot, r} \epsilon_r \right) 
      \left(\sum_{s = 1}^j A_{\cdot, s} \epsilon_s \right)^T
    \right] - 
    E[X_i]E[X_j]
  \\
  &= \sum_{t = 1}^{\min (i, j)} A_{\cdot, t}A_{\cdot, t}^T
  \\
  &= \sum_{t = 1}^{\min (i, j)} \Xi_t,
\end{align}
where many terms are 0 because the $\epsilon_i$ are independent.
The covariance matrix is
\begin{align}
  K &= 
    \begin{pmatrix}
      \Xi_1 & \Xi_1 & \cdots & \cdots & \Xi_1 
      \\
      \Xi_1 & \Xi_1 + \Xi_2 & \Xi_1 + \Xi_2 & \cdots & \Xi_1 + \Xi_2
      \\
      \Xi_1 & \Xi_1 + \Xi_2 & \ddots        &        & \vdots
      \\
      \vdots &  \vdots &    & \sum_{t=1}^{n-1} \Xi_t &  \sum_{t=1}^{n-1} \Xi_t  
      \\
      \Xi_1 & \Xi_1 + \Xi_2 & \cdots   & \sum_{t=1}^{n-1} \Xi_t & \sum_{t=1}^{n} \Xi_t
    \end{pmatrix}
\end{align}

# EP factor updates

We approximate the $t_k(x)$ with unnormalized Gaussian densities with $\tilde{S}_k$, $\tilde{\mu}_k$, and $\tilde{\sigma}^2_k$ the approximating normalizing constant, mean, and variance for the $k$th factor:
<!--  -->
\begin{align*}
  \tilde{t}_k(x) = \tilde{S}_k N(c_k^T x; \tilde{\mu}_k, \tilde{\sigma}_k)
\end{align*}
<!--  -->
leading to the overall approximating unnormalized Gaussian with normalizing
constant $S$, mean $\mu$, and covariance $\Sigma$:
<!-- . -->
\begin{align*}
  q(x) = p_0(x) \prod_{k=1}^n \tilde{t}_k(x) = N(x; \mu, \Sigma)
\end{align*}
<!-- . -->
Initialize $\mu = m$, $\Sigma = K$.
The EP cavity distribution is singular Gaussian
<!-- . -->
\begin{align}\label{eq:cavity}
  q^{\backslash k}(x) &= Z^{\backslash k} N(x; u^{\backslash k}, V^{\backslash k})
  \\
  V^{\backslash k} &= \left(\Sigma^{-1} -
    \frac{1}{\tilde{\sigma}_k}c_k c_k^T\right)^{-1}
  \\
  u^{\backslash k} &= V^{\backslash k} \left( 
    \Sigma^{-1}\mu -  \frac{\tilde{\mu}_k}{\tilde{\sigma}_k^2} c_k \right)
\end{align}
<!-- . -->
Since we are dealing with rank-one factors and the approximating factors are Gaussians and therefore closed under marginalization, we can moment match the tilted and approximating densities in the one dimension of interest, where the cavity distribution is univariate Gaussian:
\begin{align}\label{eq:marginal_cavity}
  q_{\backslash k}(c_k^Tx) = q_{\backslash k}(x_{k'}) &= S^{\backslash k}N(x_{k_k}; 
                                          \mu_{\backslash k}, 
                                          \sigma_{\backslash k})
  \\
  \sigma_{\backslash k} &= \left(\frac{1}{\Sigma_{k'k'}} - \frac{1}{\tilde{\sigma}_k^2} \right)^{-1}
  \\
  \mu_{\backslash k} &= \sigma_{\backslash k} 
    \left( \frac{\mu_{k'}}{\Sigma_{k', k'}} -
    \frac{\tilde{\mu}_k}{\tilde{\sigma}_k^2}\right)
  \\
  k' &= m(n-1) + k
\end{align}

The moments of the tilted distribution $t_k(x)q_{\backslash k}(x)$ are
\begin{align}\label{eq:moments_tilted}
  \hat{S}_k &= \frac{1}{2}\big(\Phi(\beta) - \Phi(\alpha))
  \\
  \hat{\mu}_k &= \mu_{\backslash k} + 
    \frac{1}{\hat{S}_k} \frac{\sigma_{\backslash k}}{\sqrt{2\pi}} 
    (\exp(-\alpha^2) - \exp(-\beta^2))
  \\
  \hat{\sigma}_k^2 &= \mu_{\backslash k}^2 + \sigma_{\backslash k} +
    \frac{1}{\hat{S}_k} \frac{\sigma_{\backslash k}}{\sqrt{2\pi}} 
    \left( -(b_k + \mu_{\backslash k})\exp(-\beta^2) \right) - \hat{\mu}_k^2
  \\
  \alpha &= \frac{\ell_k - \mu_{\backslash k}}{\sqrt{2 \sigma^2_{\backslash k}}}
  \\
  \beta &= \frac{u_k - \mu_{\backslash k}}{\sqrt{2 \sigma^2_{\backslash k}}}
\end{align}
We match the moments of $\tilde{t}_k(x)q_{\backslash k}(x)$ to the moments of
$t_k(x)q_{\backslash k}(x)$, leading to factor updates:
\begin{align}\label{eq:factor_update}
  \tilde{\sigma}_k^2 &= (\hat{\sigma_k}^{-2} - \sigma_{\backslash k}^{-2})^{-1}
  \\
  \tilde{\mu}_k &= 
    \tilde{\sigma}_k^2(\hat{\sigma}_k^{-2} \hat{\mu}_k - 
    \sigma_{\backslash k}^{-2} \mu_{\backslash k})
  \\
  \tilde{S}_k &= \hat{S}_k\sqrt{2\pi} 
    \sqrt{\sigma_{\backslash k}^2 + \tilde{\sigma}_k^2}
    \exp \left( \frac{1}{2} \frac{(\mu_{\backslash k} - \tilde{\mu}_k)^2}
                     {( \sigma_{\backslash k}^2 + \tilde{\sigma}_k^2 )}         
         \right)
\end{align}

# Overall approximation update

We parameterize the approximate factors in the natural parameterization as in Cunningham et al.:
\begin{align*}
  \tilde{\tau}_k &= \frac{1}{\tilde{\sigma}^2_k}
  \\
  \tilde{\nu}_k &= \frac{\tilde{\mu}_k}{\tilde{\sigma}^2_k}
\end{align*}
We update only the parts of the approximate parameters that are affected by the factor updates. 
<!-- This is the bottom right $m \times m$ corner of $\Sigma$, $\Sigma_{(mm)}$, and the last $n$ elements of $\mu$, $\mu_{(m)}$.   -->
Unfortunately this is all elements of the approximate parameters in general.
\begin{align}
  \Sigma^{new} &= \Sigma^{old} - 
  \left(
    \frac{\Delta \tilde{\tau}_k}{1 + \Delta \tilde{\tau}_k \Sigma^{old}_{k'k'}}
  \right)(\Sigma^{old}_{\cdot, k'})(\Sigma^{old}_{\cdot, k'})^T
  \\
  \mu^{new} &= \mu^{old} + 
    \left(
      \frac{\Delta \tilde{\nu}_k - \Delta \tilde{\tau}_k \mu^{old}_{k'}}
           {1 + \Delta \tilde{\tau}_k \Sigma^{old}_{k'k'}}
    \right)
    \Sigma^{old}_{\cdot, k'}
\end{align}
We also need to update the approximate precision $\Sigma^{-1}$, which we need to compute the normalizing constant of the approximate Gaussian. 
\begin{align*}
  \Sigma^{-1} = K^{-1} + \sum_{k=1}^m \tilde{\tau}_k c_k c_k^T
\end{align*}
Next we update the normalizing constant of the approximation:
\begin{align}
  \log S &= -\frac{1}{2} \sum_{k=1}^n \log |\lambda_k|  \nonumber
  \\
         & \quad  +\sum_{k=1}^n \log \tilde{S}_k - \frac{1}{2}\left(\frac{\tilde{\mu}_k^2}{\tilde{\sigma}_k^2} + \log \tilde{\sigma}_k^2 + \log(2\pi) \right) \nonumber\
          \\
        & \quad  + \frac{1}{2}\left( \mu^T\Sigma^{-1}\mu + \log |\Sigma| \right) \nonumber
  \\
  &= -\frac{1}{2} \sum_{k=1}^n \log |\lambda_k| \nonumber
  \\
  & \quad + \sum_{k=1}^n \log \hat{S}_k + 
    \frac{1}{2} \log(\tilde{\tau}_k \sigma_{\backslash k}^2 + 1) +
    \frac{1}{2} \frac{
      \mu_{\backslash k}^2 \tilde{\tau}_k  - 
      2 \mu_{\backslash k} \tilde{\nu}_k - 
      \tilde{\nu}_k^2\sigma_{\backslash k}^2
      }{
      1 + \tilde{\tau}_k \sigma_{\backslash k}^2  
      } \nonumber
  \\
  &\quad  + \frac{1}{2}\left( \mu^T\Sigma^{-1}\mu + \log |\Sigma| \right)
\end{align}
where we rewrite the middle term as Cunningham does. 
$S$ is the desired probability $P(AZ \le b)$.

# Algorithm

\begin{algorithm}[H]
  \caption{Sequence corrected EP for Gaussian probability}
  \label{algo}
  \begin{algorithmic}
    \State Set $\tilde{\sigma}_k = 0$
    \While{not converged}
      \For{$k = 1:n$}
        \State Form cavity distribution 
          $ 
            q^{\backslash k}(x) = 
              Z^{\backslash k} N(x; u^{\backslash k}, V^{\backslash k}) 
          $
        \State Compute moments of tilted distribution $t_k(x)q_{\backslash k}(x)$
        \State Update parameters of $\tilde{t}_k(x)$
      \EndFor
      \State Update overall approximation $\mu$, $\Sigma$
    \EndWhile
    \State Calculate $S$\\
    \Return $S$
  \end{algorithmic}
\end{algorithm}

# Appendix {#Appendix}

## Normalizing constants

\begin{align*}
  p_0(x) \prod_{i=1}^n \tilde{t}_i(x) &= 
    \frac{1}{\sqrt{2\pi|K|}} \exp \left( -\frac{1}{2}(x - m)^T K^{-1} (x -m\right) 
    \prod_{i=1}^n \frac{1}{\sqrt{2\pi \tilde{\sigma}^2_i}} \exp\left(-\frac{1}{2\tilde{\sigma}_i^2}(c_i^Tx - \tilde{\mu_i})^2 \right) 
    \\
    &= \exp\left( -\frac{1}{2}(n\log 2\pi + \log|K| + m^TK^{-1}m) \right)
      \exp(m^TK^{-1}x - \frac{1}{2} x^TK^{-1}x) \times
      \\
    & \quad \prod_{i=1}^n \left( 
      \exp \left( -\frac{1}{2}(\log 2\pi - \log \tilde{\tau}_i + \tilde{\nu}_i^2\tilde{\tau}_i) \right)
      \exp \left( \tilde{\nu}_i c_i^Tx - \frac{1}{2} \tilde{\tau}_i x^T c_i c_i^T x \right) \right) 
    \\
    &= \exp\left( -\frac{1}{2}(n\log 2\pi + \log|K| + m^TK^{-1}m) \right) \times
    \\
    & \quad \prod_{i=1}^n \left(
      \exp \left( -\frac{1}{2}(\log 2\pi - \log \tilde{\tau}_i + \tilde{\nu}_i^2\tilde{\tau}_i) \right) 
        \exp \left( (m^T K^{-1} + \sum_{i=1}^n \tilde{\nu}_i c_i^T) x
        - \frac{1}{2}x^T( K^{-1} + \sum_{i=1}^n \tilde{\tau}_i c_i c_i^T )x\right)
      \right)
\end{align*}
We see that this is a multivariate Gaussian distribution in canonical parameterization, with 
\begin{align}
  \eta = K^{-1}m + \sum_{i=1}^n \tilde{\nu}_i c_i
  \\
  \Lambda = K^{-1} + \sum_{i=1}^n \tilde{\tau}_i c_i c_i^T
\end{align}
and log normalizing constant
\begin{align}
  -\frac{1}{2}(n\log 2\pi + \log|K| + m^TK^{-1}m) + 
  \sum_{i=1}^n -\frac{1}{2}(\log 2\pi - \log \tilde{\tau}_i + \tilde{\nu}_i^2\tilde{\tau}_i)
\end{align}

